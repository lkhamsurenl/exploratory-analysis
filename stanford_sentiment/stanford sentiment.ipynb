{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment classification \n",
    "\n",
    "Using https://ai.stanford.edu/~amaas/data/sentiment/ we perform simple sentiment classification. \n",
    "\n",
    "The notebook is supposed to demonstrate quick ML development process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data can be found in https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "import urllib.request\n",
    "from time import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    urllib.request.urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', file_name)\n",
    "\n",
    "# unzip if not exists\n",
    "if not os.path.exists('aclImdb'):\n",
    "    !tar -xvzf {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "In our case, the problem is simple binary classification problem with positive or negative review and our dataset\n",
    "is balanced, so for MVP we propose to use accuracy as main metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of pos files available: 12500\n"
     ]
    }
   ],
   "source": [
    "full_train_dir = 'aclImdb/train'\n",
    "full_train_pos_dir = f'{full_train_dir}/pos'\n",
    "full_train_neg_dir = f'{full_train_dir}/neg'\n",
    "full_train_pos_filenames = [f for f in listdir(full_train_pos_dir) if isfile(join(full_train_pos_dir, f))]\n",
    "full_train_neg_filenames = [f for f in listdir(full_train_neg_dir) if isfile(join(full_train_neg_dir, f))]\n",
    "\n",
    "print(f'total number of pos files available: {len(full_train_pos_filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_filenames=20000\n",
      "valid_filenames=5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_num_files = 10000\n",
    "\n",
    "filenames = full_train_pos_filenames + full_train_neg_filenames\n",
    "labels = [1.0 for _ in range(len(full_train_pos_filenames))] + [0.0 for _ in range(len(full_train_neg_filenames))]\n",
    "train_filenames, valid_filenames, train_y, valid_y = train_test_split(filenames, labels, test_size=0.2)\n",
    "\n",
    "print(f\"\"\"\n",
    "train_filenames={len(train_filenames)}\n",
    "valid_filenames={len(valid_filenames)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_content(filenames, labels):\n",
    "    texts = []\n",
    "    for i, name in enumerate(filenames):\n",
    "        subdir = 'pos' if labels[i] == 1.0 else 'neg'\n",
    "        with open(f'{full_train_dir}/{subdir}/{name}', 'r') as f:\n",
    "            texts.append(f.read())\n",
    "            \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_texts = get_content(train_filenames, train_y)\n",
    "valid_texts = get_content(valid_filenames, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_texts=20000; train_y=20000\n"
     ]
    }
   ],
   "source": [
    "print(f'train_texts={len(train_texts)}; train_y={len(train_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_X = vectorizer.transform(valid_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': (0.25, 0.5),\n",
    "    'vect__max_features': (None, 50000),\n",
    "    'vect__ngram_range': ((1, 2),),  # unigrams or bigrams\n",
    "    'tfidf__norm': ('l2',),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__penalty': ('l2',),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000, 10000, 50000), 'vect__ngram_range': ((1, 1), (1, 2)), 'tfidf__norm': ('l2',), 'clf__max_iter': (50, 100), 'clf__penalty': ('l2',)}\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 28.1min finished\n",
      "/Users/lkhamsurenl/Library/Python/3.7/lib/python/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1704.964s\n",
      "\n",
      "Best score: 0.892\n",
      "Best parameters set:\n",
      "\tclf__max_iter: 50\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: 50000\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Validation report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.94      0.95      9970\n",
      "         1.0       0.94      0.96      0.95     10030\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "Validation report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.89      0.90      2530\n",
      "         1.0       0.89      0.90      0.90      2470\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train_texts, train_y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "train_y, train_preds = train_y, grid_search.predict(train_texts)\n",
    "print('Validation report:')\n",
    "print(classification_report(train_y, train_preds))\n",
    "\n",
    "\n",
    "valid_y, valid_preds = valid_y, grid_search.predict(valid_texts)\n",
    "print('Training report:')\n",
    "print(classification_report(valid_y, valid_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see model is definitely overfitting into the current training set. There are couple things to note:\n",
    "1. Even with simple logistic regression, we are already overfitting into the model. More sophisticated model with current dataset is likely to overfit even further (MLP, tree based models)\n",
    "2. Having more data likely to help us generalize better\n",
    "3. We should try out better regularization (we have used L2 above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Some ideas around improving the performance of the model:\n",
    "1. Simplify our features, right now tf-idf seems to have too many features\n",
    "2. Can we improve classification accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
